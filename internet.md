[[LINUX]]
# internet

- search smarter by [dorking](dorking)
- retrieving information from websites:
    - https://web.archive.org/
    - https://archive.ph/
    - Websites indicate how scrapers and search engines should interact with
      their content by using a file called **“robots.txt”**.
    - **"sitemap.xml"** files are sort of the opposite to the robots.txt files. They are
      used by site administrators to inform search engines about pages on their
      site that are available for crawling.
- what is behind a website:
    - whois:
        - https://iana.org/whois
        - https://who.is/
        - https://www.whois.com/whois/
        - https://godaddy.com/whois
        - https://whois.domaintools.com/
    - 
